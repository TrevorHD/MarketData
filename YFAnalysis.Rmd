---
title: 'Machine Learning and Stock Market Data'
subtitle: \textit{An Exercise In Predictive Modelling}
author: "Trevor H. Drees"
output:
  pdf_document:
    fig_caption: yes
    toc: true
editor_options: 
  chunk_output_type: console
header-includes:
- \usepackage{float}
- \usepackage{booktabs}
- \usepackage[width = 0.96\textwidth]{caption}
- \usepackage[hang, flushmargin]{footmisc}
urlcolor: blue
---

\setlength{\skip\footins}{0.35in}

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE)

# Load libraries
library(MASS)
library(corrplot)
library(plyr)
library(tidyverse)
library(reshape2)
library(knitr)
library(pROC)
library(car)

# Tidy data
source("https://raw.githubusercontent.com/TrevorHD/MarketData/master/YFData.R")

# Set seed for (pseudo) RNG so results can be replicated
#RNGkind(sample.kind = "Rounding")
set.seed(18361)

```

\newpage

# Disclaimer

Trading stocks, ETFs, options, futures, or any other financial assets or derivatives has inherent risks that should be understood before making any substantial investments; such risks can include significant loss of principal, and it is the investor's responsibility to determine their appropriate level of exposure to risk. The contents of this work are not, in any form, meant to serve as investment advice, but rather as exploratory analysis of stock market data with the goal being able to predict changes in future conditions based solely off of conditions at a prior date. As such, the author bears no liability for any damages or losses, realised or unrealised, incurred by the reader should they choose to use the models in these analyses for any sort of investment guidance.

# Introduction

Stock market data can be incredibly difficult to predict from just stock prices alone; changes in value of a particular stock or ETF today, last week, or last month do not necessarily dictate how it will move today. Often, fluctuations in value are instead driven by shifts in investor confidence due to changes in government monetary policy, outlook on a company's so-called "fundamentals", or disruptions in activity of a particular company or sector, among many other possible factors. While a well-diversified portfolio will almost certainly increase in value over the long run without frequent intervention, the ability to predict short-term changes and fluctuations based off market data is incredibly useful to active traders that are less concerned with holding long positions. For investors that hold particular investments over the course of only days or weeks, short-term changes are much more important for deciding when to buy and sell a stock or whether to make a put or call when trading options.

Here, we seek to create models that can predict short-term market behaviour, as such a model would be a very valuable investment tool for traders. More specifically, seek to be able to predict whether the market will increase or decrease a) in a given day, using the performance of the previous 5 trading days and b) in a given week, using the performance of the previous 5 weeks (25 trading days). To inform inform our models, we use data from three major U.S. stock market indices: the S&P 500, NASDAQ Composite, and Dow Jones Industrial Average. For each index and each timeframe, we compare models derived from a variety of classification frameworks, including parametric methods such as logistic regression and discriminant analysis as well as non-parametric methods such as $k$-nearest neighbours and support vector machines. We also discuss possible trading strategies based on model attributes such as overall accuracy, sensitivity, specificity, and positive and negative predictive value.

# Predictive Analysis: S&P 500

## Data

We first start by examining weekly data on the S&P 500, a major stock market index consisting of 500 publicly-traded American companies. This data, available online from Yahoo Finance[^FootNote1], was scraped in Python using the `yfinance` module and stored in the file `SPWeekly`; weekly percent change was then calculated using the opening and closing prices for a particular week, and lagged up to 5 weeks. The total and average percent change over a 5-week period were also calculated. The variables in our new dataset are listed below:

* **BVolume**: average number of daily shares traded (billions)
* **PctChange**: percent change for a given week
* **Direction**: whether the percent change for a given week was negative or positive
* **Prev5GM**: Average percent increase over the previous 5 weeks, calculated using the geometric mean
* **Prev5Pct**: Total percent increase over the previous 5 weeks
* **Lag1**: 1-week lag on percent change for a given week
* **Lag2**: 2-week lag on percent change for a given week
* **Lag3**: 3-week lag on percent change for a given week
* **Lag4**: 4-week lag on percent change for a given week
* **Lag5**: 5-week lag on percent change for a given week
* **VLag1**: 1-week lag on volume for a given week
* **VLag2**: 2-week lag on volume for a given week
* **VLag3**: 3-week lag on volume for a given week
* **VLag4**: 4-week lag on volume for a given week
* **VLag5**: 5-week lag on volume for a given week

We first conduct some exploratory analyses of the data to see if there are any patterns that need attention or may be important for our analyses. Upon examining the correlation matrix and heatmap shown in Figure \ref{fig:Figure1}, there seem to be several strong correlations. The strong positive correlation between `Direction` and `PctChange` is not surprising, given that `Direction` is literally defined by whether the percent change is negative or positive. Though we would thus expect there to be a perfect relationship between the two variables, and indeed there is, it does not translate well to a linear correlation. We also see that `Prev5GM` and `Prev5Pct` have a correlation of 1; this is because the total percent increase over the previous 5 weeks can also be found by exponentiating the average percent increase to the fifth power. Thus, there is an obvious relationship between the two variables, and any models containing the both may experience multicollinearity issues.

Another thing worth pointing out in \ref{fig:Figure1} is that `Direction`, the variable that we're interested in predicting, bears almost no correlation with `Prev5GM`, `Prev5Pct`, or any of the `Lag` variables; this may be problematic when trying to make predictions. Figure \ref{fig:Figure2} provides further evidence of this, focusing specifically on the lack of correlation between percent change in a given week and the five lag variables. Here, we see that there is almost no autocorrelation of percent change in a given week; that is, percent change in a given week is not strongly correlated with percent change in any of the five weeks before. This observation may be bad news for our analyses later: if there is not much of an association between percent change in a given week and percent change in any of the weeks before it, then how can we use past weekly performance to accurately predict future weekly performance?

Separating the data based on the value of `Direction` allows us to further explore trends in weekly increases and decreases. Upon doing so, we see that the mean weekly percent decrease is -1.81%, while the mean weekly increase is 1.67%; however, even though the average magnitude of weekly percent decreases is higher than that of weekly percent increases, the number of weeks in which there was an increase is greater than the number of weeks in which there was a decrease (1041 versus 784). The fact that there are more positive than negative weeks is likely responsible for the trend shown in Figure \ref{fig:Figure3} where, despite plummeting several times and suffering numerous smaller drops, the S&P 500 was still almost 14 times as high in 2021 as it was in 1986.

We now move forward with our analyses, keeping in mind that the lack of autocorrelation in weekly percent change may make it difficult to accurately construct models that use past percentage changes to predict future S&P 500 movements.

[^FootNote1]: Data available at https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC.

``` {r include = FALSE}

# Counts and means for decreases/increases
SPWeekly %>% group_by(Direction) %>% 
  summarise(Count = n(),
            Change = mean(PctChange))

# Split data into training and test set
trainVec <- sample(1:nrow(SPWeekly), size = nrow(SPWeekly)*0.7, replace = FALSE)
SPWeekly <- SPWeekly[SPWeekly$Direction != "Zero", ]
SPWeekly$Direction <- factor(SPWeekly$Direction)
SPWeeklyTrain <- SPWeekly[trainVec, ]
SPWeeklyTest <- SPWeekly[-trainVec, ]

```

``` {r Figure1, fig.height = 8, fig.cap = "\\label{fig:Figure1} Correlations bewteen year, volume, percent change, and the various lag variables for the `Weekly` data on the S&P 500. The area of squares below the diagonal are proportional to the absolute value of the corresponding correlation coefficients."}

# Plots of correlations between variables: weekly data
par(mfrow = c(2, 1), mar = c(5, 4, 0.2, 2) + 0.1, mgp = c(2.3, 0.6, 0))
SPWeekly %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(method = "square", type = "lower", win.asp = 0.7,
         tl.pos = "tp", tl.col = "black")
SPWeekly %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(add = TRUE, method = "number", type = "upper", 
         number.cex = 0.8, col = "black", tl.pos = "n", cl.pos = "n")

# Plots of correlations between variables: daily data
par(mar = c(5, 4, 0, 2) + 0.1)
SPDaily %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(method = "square", type = "lower", win.asp = 0.7,
         tl.pos = "tp", tl.col = "black")
SPDaily %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(add = TRUE, method = "number", type = "upper", 
         number.cex = 0.8, col = "black", tl.pos = "n", cl.pos = "n")

```

``` {r Figure2, fig.height = 8, fig.cap = "\\label{fig:Figure2} Plots of S&P 500 weekly percent change against percent change of up to 5 weeks prior. Autocorrelation in weekly percent change corresponding to 1-5 week lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Note: excludes the 1 week where market dropped almost 40%
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(SPWeekly$PctChange, SPWeekly$Lag1, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(20, -20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (1-Week Lag)")
plot(SPWeekly$PctChange, SPWeekly$Lag2, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(20, -20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (2-Week Lag)")
plot(SPWeekly$PctChange, SPWeekly$Lag3, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(20, -20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (3-Week Lag)")
plot(SPWeekly$PctChange, SPWeekly$Lag4, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(20, -20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (4-Week Lag)")
plot(SPWeekly$PctChange, SPWeekly$Lag5, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(20, -20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (5-Week Lag)")
acf(SPWeekly$PctChange, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure3, fig.height = 3.9, fig.cap = "\\label{fig:Figure3} Relative value of the S&P 500 over time compared to its 1986 value, as indicated by the black line. The chart on the bottom represents positive (green) and negative (red) weekly percent changes."}

# Check this chart for accuracy, as it doesn't quite match up with the YF chart
par(mar = c(5, 4, 2, 2) + 0.1)
colourList <- replace(rep("green", nrow(SPWeekly)), which(SPWeekly$PctChange < 0), "red")
barplot(abs(SPWeekly$PctChange), col = colourList, border = colourList, ylim = c(0, 50), axes = FALSE)
par(new = TRUE)
totalReturn <- c()
for(i in 1:nrow(SPWeekly)){
  totalReturn <- c(totalReturn, prod((SPWeekly$PctChange[1:i]/100)+1))}
plot(1:nrow(SPWeekly), totalReturn, type = "l", ylim = c(0, 20), xaxt = "n", 
     xlab = NA, ylab = "Proportion of Initial Value")
axis(1, at = nrow(SPWeekly)*(0:7)/7, labels = seq(1986, 2021, length.out = 8))
title(xlab = "Year")

```

## Parametric Methods

### Logistic Regression

``` {r include = FALSE}

# Note: Our analyses show that VLag1 - VLag5 doesn't make much of a difference
# Thus, we remove it to make the models simpler
select(SPWeeklyTrain, !(VLag1:VLag5)) -> SPWeeklyTrain
select(SPWeeklyTest, !(VLag1:VLag5)) -> SPWeeklyTest

# Function to perform logistic regression and output predictive accuracy
logreg <- function(formula, dataTrain, dataTest, output){
  
  # Fit logistic regression using training data
  glm.fit <- glm(formula, data = dataTrain, family = "binomial")
  
  # Classify test data; get posterior probabilities
  glm.probs <- predict(glm.fit, dataTest, type = "response")
  glm.pred <- rep("Down", length(glm.probs))
  glm.pred[glm.probs >= 0.5] <- "Up"
  
  # Confusion matrix of classified observations
  # Must manually add Down row if model never predicts Down
  confMatrix <- table(glm.pred, dataTest$Direction)
  if(nrow(confMatrix) == 1){
    confMatrix <- rbind(c(0, 0), confMatrix)}
  rownames(confMatrix) <- c("[P]Down", "[P]Up")
  colnames(confMatrix) <- c("[O]Down", "[O]Up")
  
  # Overall predictive accuracy
  accuracy <- sum(diag(confMatrix)/sum(confMatrix))
  
  # Sensitivity and specificity
  specificity <- confMatrix[1, 1]/sum(confMatrix[, 1])
  sensitivity <- confMatrix[2, 2]/sum(confMatrix[, 2])
  
  # Positive and negative predictive value
  npv <- confMatrix[1, 1]/sum(confMatrix[1, ])
  ppv <- confMatrix[2, 2]/sum(confMatrix[2, ])
  
  # Return individual outputs or list of all outputs
  if(output == "all"){
    return(list(confMatrix = confMatrix, accuracy = accuracy, specificity = specificity,
              sensitivity = sensitivity, npv = npv, ppv = ppv))}
  if(output == "confMatrix"){
    return(confMatrix)}
  if(output == "accuracy"){
    return(accuracy)}
  if(output == "specificity"){
    return(specificity)}
  if(output == "sensitivity"){
    return(sensitivity)}
  if(output == "npv"){
    return(npv)}
  if(output == "ppv"){
    return(ppv)}}

```

``` {r include = FALSE}

# WORK IN PROGRESS

# Function to fit logistic regression to all possible combinations of n variables
# Outputs value and combination of variables for desired prediction metric
logreg.comb <- function(dataTrain, dataTest, output, n){
  combn(names(dataTrain)[-c(2:3)], n) %>% 
    apply(FUN = paste0, MARGIN = 2, collapse = "+") %>% 
    paste0("Direction~", .) %>% 
    sapply(FUN = logreg, dataTrain = dataTrain, dataTest = dataTest, output = output) %>% 
    sort() %>%
    return()}

# Find top 10 models for accuracy
SPWeeklyTop <- unlist(sapply(1:8, FUN = logreg.comb, dataTrain = SPWeeklyTrain,
                      dataTest = SPWeeklyTest, output = "accuracy"))
sort(SPWeeklyTop, decreasing = TRUE)[1:10]

# Code above can be modified to get top 10 models for sensitivity, specificity, and NPV
# Not particularly useful, so they won't be used much in our analyses

# Get statistics from top 10 models in terms of accuracy
SPWeeklyTopA <- lapply(names(sort(SPWeeklyTop, decreasing = TRUE)[1:10]),
                       FUN = glm, data = SPWeeklyTrain, family = "binomial")

# Get coefficients from top 10 models
lapply(SPWeeklyTopA, FUN = coef) %>% 
  lapply(FUN = as.list) -> coeflist

# Create table of coefficients, AIC, and accuracy
SPWeeklyTopATab <- cbind(do.call(rbind.fill, lapply(coeflist, as.data.frame)),
                         sapply(SPWeeklyTopA, FUN = AIC),
                         as.numeric(sort(SPWeeklyTop, decreasing = TRUE)[1:10]))
names(SPWeeklyTopATab)[c(1, 9, 10, 11)] <- c("Intercept", "Lag1", "AIC", "Accuracy")

# Calculate maximum VIF for each model
sapply(sapply(SPWeeklyTopA, FUN = vif), FUN = max)

# Reorder table columns
SPWeeklyTopATab <- SPWeeklyTopATab[, c(1, 9, 2:4, 8, 5:7, 10:11)]

# Find top 10 models for PPV
SPWeeklyTop <- unlist(sapply(1:8, FUN = logreg.comb, dataTrain = SPWeeklyTrain,
                      dataTest = SPWeeklyTest, output = "ppv"))
sort(SPWeeklyTop, decreasing = TRUE)[1:10]

# Get statistics from top 10 models in terms of PPV
SPWeeklyTopP <- lapply(names(sort(SPWeeklyTop, decreasing = TRUE)[1:10]),
                       FUN = glm, data = SPWeeklyTrain, family = "binomial")

# Get coefficients from top 10 models
lapply(SPWeeklyTopP, FUN = coef) %>% 
  lapply(FUN = as.list) -> coeflist

# Create table of coefficients, AIC, max VIF, and accuracy
SPWeeklyTopPTab <- cbind(do.call(rbind.fill, lapply(coeflist, as.data.frame)),
                         sapply(SPWeeklyTopP, FUN = AIC),
                         as.numeric(sort(SPWeeklyTop, decreasing = TRUE)[1:10]))
names(SPWeeklyTopPTab)[c(1, 10, 11)] <- c("Intercept", "AIC", "PPV")

# Calculate maximum VIF for each model
sapply(sapply(SPWeeklyTopP, FUN = vif), FUN = max)

# Reorder table columns
SPWeeklyTopPTab <- SPWeeklyTopPTab[, c(1, 8, 2:3, 6:7, 9, 4, 5, 10:11)]

# Remove unneeded variables
remove(SPWeeklyTop, coeflist)

# Direction ~ Lag2 + Lag3 + Prev5Pct + Volume seems like the best candidate
# 2nd highest accuracy and highest PPV

```

``` {r}

# Print tables of top 10 logistic models for accuracy and PPV
options(knitr.kable.NA = '')
kable(round(SPWeeklyTopATab, 3))
kable(round(SPWeeklyTopPTab, 3))

```

``` {r Figure4, eval = FALSE, fig.height = 8, fig.cap = "\\label{fig:Figure4} Standardised deviance residuals (top) and standardised Pearson residuals (bottom) for the best model in the logistic regression framework. Solid red lines represent a loess fit to the residuals and should be approximately horizontal at zero; dotted red lines represent $\\pm$ 2 standard deviations on the residuals."}

# Re-fit best model for easy plotting
glm.fit <- glm(Direction ~ Lag2 + Lag3 + Prev5Pct + Volume, data = SPWeeklyTrain, family = "binomial")

# Calculate standardised deviance and Pearson residuals for logistic regression
hvals <- influence(glm.fit)$hat
SRDeviance <- rstandard(glm.fit)
SRPearson <- residuals(glm.fit, "pearson")/sqrt(1 - hvals)

# Plot standardised deviance residuals
par(mfrow = c(2, 1), mar = c(5, 4, 0.2, 2) + 0.1, mgp = c(2.3, 0.6, 0))
plot(predict(glm.fit, type = "response"), SRDeviance, xlab = "Fitted Probability",
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylim = c(-3, 3),
     ylab = "Standardised Deviance Residuals")
lines(lowess(predict(glm.fit), residuals(glm.fit)), col = "red", lwd = 1.5)
abline(h = c(-2, 2), col = "red", lty = 3, lwd = 1.5)

# Plot standardised Pearson residuals
plot(predict(glm.fit, type = "response"), SRPearson, xlab = "Fitted Probability",
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylim = c(-3, 3),
     ylab = "Pearson Deviance Residuals")
lines(lowess(predict(glm.fit), residuals(glm.fit, "pearson")), col = "red", lwd = 1.5)
abline(h = c(-2, 2), col = "red", lty = 3, lwd = 1.5)

```

### Discriminant Analysis

## Non-Parametric Methods

### K-Nearest Neighbours

### Support Vector Machines

# Predictive Analysis: NASDAQ Composite

## Data

## Parametric Methods

### Logistic Regression

### Discriminant Analysis

## Non-Parametric Methods

### K-Nearest Neighbours

### Support Vector Machines

# Predictive Analysis: Dow Jones Industrial Average

## Data

## Parametric Methods

### Logistic Regression

### Discriminant Analysis

## Non-Parametric Methods

### K-Nearest Neighbours

### Support Vector Machines

# Conclusions
