---
title: 'Machine Learning and Stock Market Data'
subtitle: \textit{An Exercise In Predictive Modelling}
author: "Trevor H. Drees"
output:
  pdf_document:
    fig_caption: yes
    toc: true
editor_options: 
  chunk_output_type: console
header-includes:
- \usepackage{float}
- \usepackage{booktabs}
- \usepackage[width = 0.96\textwidth]{caption}
- \usepackage[hang, flushmargin]{footmisc}
urlcolor: blue
---

\setlength{\skip\footins}{0.35in}

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE)

# Load libraries
library(MASS)
library(corrplot)
library(plyr)
library(tidyverse)
library(reshape2)
library(knitr)
library(pROC)
library(car)

# Tidy data
source("https://raw.githubusercontent.com/TrevorHD/MarketData/master/YFData.R")

# Set seed for (pseudo) RNG so results can be replicated
set.seed(18361)

```

\newpage

# Disclaimer

Trading stocks, ETFs, options, futures, or any other financial assets or derivatives has inherent risks that should be understood before making any substantial investments; such risks can include significant loss of principal, and it is the investor's responsibility to determine their appropriate level of exposure to risk. The contents of this work are not, in any form, meant to serve as investment advice, but rather as exploratory analysis of stock market data with the goal being able to predict changes in future conditions based solely off of conditions at a prior date. As such, the author bears no liability for any damages or losses, realised or unrealised, incurred by the reader should they choose to use the models in these analyses for any sort of investment guidance.

# Introduction

Stock market data can be incredibly difficult to predict from just stock prices alone; changes in value of a particular stock or ETF today, last week, or last month do not necessarily dictate how it will move today. Often, fluctuations in value are instead driven by shifts in investor confidence due to changes in government monetary policy, outlook on a company's so-called "fundamentals", or disruptions in activity of a particular company or sector, among many other possible factors. While a well-diversified portfolio will almost certainly increase in value over the long run without frequent intervention, the ability to predict short-term changes and fluctuations based off market data is incredibly useful to active traders that are less concerned with holding long positions. For investors that hold particular investments over the course of only days or weeks, short-term changes are much more important for deciding when to buy and sell a stock or whether to make a put or call when trading options.

Here, we seek to create models that can predict short-term market behaviour, as such a model would be a very valuable investment tool for traders. More specifically, seek to be able to predict whether the market will increase or decrease a) in a given day, using the performance of the previous 5 trading days and b) in a given week, using the performance of the previous 5 weeks (25 trading days). To inform inform our models, we use data from three major U.S. stock market indices: the S&P 500, NASDAQ Composite, and Dow Jones Industrial Average. For each index and each timeframe, we compare models derived from a variety of classification frameworks, including parametric methods such as logistic regression and discriminant analysis as well as non-parametric methods such as $k$-nearest neighbours and support vector machines. We also discuss possible trading strategies based on model attributes such as overall accuracy, sensitivity, specificity, and positive and negative predictive value.

# Data

We first start by introducing and briefly exploring the six data sets that will be used in these analyses. First, we have `SPDaily` and `SPWeekly`, which are daily and weekly data (respectively) on the S&P 500, a major stock market index consisting of 500 publicly-traded American companies. Next, we have `NDDaily` and `NDWeekly`, which are daily and weekly data (respectively) on the NASDAQ Composite, another major stock market index that is largely weighted on the information technology sector. Finally, we have `DJDaily` and `DJWeekly`, which are daily and weekly data (respectively) on the NASDAQ Composite, a major stock market index that consisting of 30 large American companies from a variety of different market sectors. All data, which are available online from Yahoo Finance[^FootNote1], were scraped in Python using the `yfinance` packeage and stored in six different CSV files; weekly/daily percent change was then calculated using the opening and closing prices for a particular week/day, and lagged up to 5 weeks/days. The total and average percent change over a 5-week or 5-day period were also calculated. The variables in our new datasets are listed below:

* **BVolume**: average number of daily shares traded (billions)
* **PctChange**: percent change for a given week/day
* **Direction**: whether the percent change for a given week/day was negative or positive
* **Prev5GM**: Average ()geometric mean) percent increase over the previous 5 weeks/days
* **Prev5Pct**: Total percent increase over the previous 5 weeks/days
* **Lag1**: 1-week lag on percent change for a given week/day
* **Lag2**: 2-week lag on percent change for a given week/day
* **Lag3**: 3-week lag on percent change for a given week/day
* **Lag4**: 4-week lag on percent change for a given week/day
* **Lag5**: 5-week lag on percent change for a given week/day
* **VLag1**: 1-week lag on volume for a given week/day
* **VLag2**: 2-week lag on volume for a given week/day
* **VLag3**: 3-week lag on volume for a given week/day
* **VLag4**: 4-week lag on volume for a given week/day
* **VLag5**: 5-week lag on volume for a given week/day

Note that for the `SPDaily`, `NDDaily`, and `DJDaily` datasets, all calculations and quantities are in terms of days rather than weeks; for the `SPWeekly`, `NDWeekly`, and `DJWeekly` datasets, all calculations and quantities are in terms of weeks rather than days. For example, the `Lag2` variable would represent the percent change two days ago in the `SPDaily` dataset, but would represent the percent change two weeks ago in the `SPWeekly` dataset.

We first conduct some exploratory analyses of the data to see if there are any patterns that need attention or may be important for our analyses, starting with the S&P 500 daily and weekly data. Upon examining the correlation matrices and heatmaps shown in Figure \ref{fig:Figure1}, there seems to be several strong correlations. The strong positive correlation between `Direction` and `PctChange` is not surprising, given that `Direction` is literally defined by whether the percent change is negative or positive. Though we would thus expect there to be a perfect relationship between the two variables, and indeed there is, it does not translate well to a linear correlation. We also see that `Prev5GM` and `Prev5Pct` have a correlation of 1; this is because the total percent increase over the previous 5 weeks can also be found by exponentiating the average percent increase to the fifth power. Thus, there is an obvious relationship between the two variables, and any models containing the both may experience multicollinearity issues. These same trends can also be seen in the NASDAQ data (Figure \ref{fig:Figure2}) and the Dow Jones data (Figure \ref{fig:Figure3}).

Another thing worth pointing out in Figure \ref{fig:Figure1} is that `Direction`, the variable that we're interested in predicting, bears almost no correlation with `Prev5GM`, `Prev5Pct`, or any of the `Lag` variables; this may be problematic when trying to make predictions. Figure \ref{fig:Figure4} provides further evidence of this on the S&P 500 weekly data and Figure \ref{fig:Figure5} on the S&P 500 daily data, focusing specifically on the lack of correlation between percent change and the five lag variables. Here, we see that there is almost no autocorrelation of percent change in a given week; that is, percent change in a given week is not strongly correlated with percent change in any of the five weeks before. This observation may be bad news for our analyses later: if there is not much of an association between percent change in a given week and percent change in any of the weeks before it, then how can we use past weekly performance to accurately predict future weekly performance? While there is no apparent autocorrelation in returns for the S&P 500, there does seem to be an autocorrelation in trading volume, as is evident in Figures \ref{fig:Figure6} and \ref{fig:Figure7}.

Unsurprisingly, we also see the exact same lack of autocorrelation in returns and strong autocorrelation in trading volume for both the NASDAQ and Dow Jones data. Figures \ref{fig:Figure8} and \ref{fig:Figure9} demonstrate a lack of autocorrelation in NASDAQ daily and weekly returns, while Figures \ref{fig:Figure10} and \ref{fig:Figure11} show that daily and weekly volume is strongly autocorrelated; likewise, Figures \ref{fig:Figure12} and \ref{fig:Figure13} also demonstrate a lack of autocorrelation in Dow Jones daily and weekly returns, while Figures \ref{fig:Figure14} and \ref{fig:Figure15} again show that daily and weekly volume is strongly autocorrelated.

Separating the S&P 500 data based on the value of `Direction` allows us to further explore trends in increases and decreases. For example, upon doing so, we see that the mean weekly percent decrease on the S&P 500 is -1.81%, while the mean weekly increase is 1.67%; however, even though the average magnitude of weekly percent decreases is higher than that of weekly percent increases, the number of weeks in which there was an increase is greater than the number of weeks in which there was a decrease (1041 versus 784). If we look at the data in terms of days instead, we find that there are again more increases than decreases, with 4063 negative days with a mean daily decrease of -0.752% and 4750 positive days with a mean daily increase of 0.707%; this difference is noticable in the bottom panel of Figure \ref{fig:Figure16}. The fact that there are more positive than negative days/weeks is likely responsible for the trend shown in the top panel of Figure \ref{fig:Figure16} where, despite plummeting several times (e.g. two recessions in the 2000's and the COVID-related plunge in early 2020) and suffering numerous smaller drops, the S&P 500 was still almost 14 times as high in 2021 as it was in 1986.

Again, we see the same trends in the NASDAQ and Dow Jones data if we separate it based on `Direction`. The NASDAQ has 772 negative weeks with a mean weekly decrease of -2.36% and 1052 positive weeks with a mean weekly increase of 2.04%; when broken into days, there 4000 negative days with a mean daily decrease of -0.849% and 4776 positive days with a mean daily increase of 0.724%. This difference is noticable in the bottom panel of Figure \ref{fig:Figure17} and is likely responsible for the increase in value over time, despite the massive crash in 2000 from the Dot-Com Bubble. If we look at the Dow Jones, we see that it has 643 negative weeks with a mean weekly decrease of -1.64% and 866 positive weeks with a mean weekly increase of 1.53%; when broken into days, there 3383 negative days with a mean daily decrease of -0.728% and 3913 positive days with a mean daily increase of 0.696%. This difference is again noticable in the bottom panel of Figure \ref{fig:Figure18} and is also likely responsible for the increase in value over time. Note that because the Yahoo Finance data on the Dow Jones only starts in 1992 (though the Dow Jones has been around much longer than that), we have less data to work with than we do for the S&P 500 and NASDAQ.

We now move forward with our analyses, keeping in mind that the lack of autocorrelation in weekly percent change may make it difficult to accurately construct models that use past percentage changes to predict future S&P 500 movements. However, there is some good news: because there is only a slight difference in the number of positive and negative days/weeks, we do not have to worry about predicting a small number of observations in a highly-skewed data set, and thus face fewer restrictions with the tools and algorithms we plan on using. Before we begin, we split each of our data sets into a training set with 70% of the observations and a test set with the remaining 30% of observations; this validation set approach will allows us to fit models on the training data and then examine their performance on the test data, reducing the chances of overfitting the data. We also drop the variables `VLag1` through `VLag5`, as preliminary analyses (not shown in this report) indicate that these variables make almost no difference when constructing our models.

[^FootNote1]: S&P 500 data can be found [here](https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC), NASDAQ Composite data can be found [here](https://finance.yahoo.com/quote/%5EIXIC/history?p=%5EIXIC), and Dow Jones Industrial Data can be found [here](https://finance.yahoo.com/quote/%5EDJI/history?p=%5EDJI). Data last accessed on 26 March 2021.

``` {r include = FALSE}

# Counts and means for S&P 500 decreases/increases
SPWeekly %>% group_by(Direction) %>% 
  summarise(Count = n(),
            Change = mean(PctChange))
SPDaily %>% group_by(Direction) %>% 
  summarise(Count = n(),
            Change = mean(PctChange))

# Counts and means for NASDAQ decreases/increases
NDWeekly %>% group_by(Direction) %>% 
  summarise(Count = n(),
            Change = mean(PctChange))
NDDaily %>% group_by(Direction) %>% 
  summarise(Count = n(),
            Change = mean(PctChange))

# Counts and means for Dow Jones decreases/increases
DJWeekly %>% group_by(Direction) %>% 
  summarise(Count = n(),
            Change = mean(PctChange))
DJDaily %>% group_by(Direction) %>% 
  summarise(Count = n(),
            Change = mean(PctChange))

# Split data into training and test set
trainVec <- sample(1:nrow(SPWeekly), size = nrow(SPWeekly)*0.7, replace = FALSE)
SPWeekly <- SPWeekly[SPWeekly$Direction != "Zero", ]
SPWeekly$Direction <- factor(SPWeekly$Direction)
SPWeeklyTrain <- SPWeekly[trainVec, ]
SPWeeklyTest <- SPWeekly[-trainVec, ]

```

``` {r Figure1, fig.height = 8, fig.cap = "\\label{fig:Figure1} Correlations bewteen volume, percent change, and the various lag variables for the weekly (top) and daily (bottom) data on the S&P 500. The area of squares below the diagonal are proportional to the absolute value of the corresponding correlation coefficients."}

# Plots of correlations between variables: weekly data
par(mfrow = c(2, 1), mar = c(5, 4, 0.2, 2) + 0.1, mgp = c(2.3, 0.6, 0))
SPWeekly %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(method = "square", type = "lower", win.asp = 0.7, tl.cex = 0.7,
         tl.pos = "tp", tl.col = "black")
SPWeekly %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(add = TRUE, method = "number", type = "upper", 
         number.cex = 0.5, col = "black", tl.pos = "n", cl.pos = "n")

# Plots of correlations between variables: daily data
par(mar = c(5, 4, 0, 2) + 0.1)
SPDaily %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(method = "square", type = "lower", win.asp = 0.7, tl.cex = 0.7,
         tl.pos = "tp", tl.col = "black")
SPDaily %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(add = TRUE, method = "number", type = "upper", 
         number.cex = 0.5, col = "black", tl.pos = "n", cl.pos = "n")

```

``` {r Figure2, fig.height = 8, fig.cap = "\\label{fig:Figure2} Correlations bewteen volume, percent change, and the various lag variables for the weekly (top) and daily (bottom) data on the NASDAQ Composite. The area of squares below the diagonal are proportional to the absolute value of the corresponding correlation coefficients."}

# Plots of correlations between variables: weekly data
par(mfrow = c(2, 1), mar = c(5, 4, 0.2, 2) + 0.1, mgp = c(2.3, 0.6, 0))
NDWeekly %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(method = "square", type = "lower", win.asp = 0.7, tl.cex = 0.7,
         tl.pos = "tp", tl.col = "black")
NDWeekly %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(add = TRUE, method = "number", type = "upper", 
         number.cex = 0.5, col = "black", tl.pos = "n", cl.pos = "n")

# Plots of correlations between variables: daily data
par(mar = c(5, 4, 0, 2) + 0.1)
NDDaily %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(method = "square", type = "lower", win.asp = 0.7, tl.cex = 0.7,
         tl.pos = "tp", tl.col = "black")
NDDaily %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(add = TRUE, method = "number", type = "upper", 
         number.cex = 0.5, col = "black", tl.pos = "n", cl.pos = "n")

```

``` {r Figure3, fig.height = 8, fig.cap = "\\label{fig:Figure3} Correlations bewteen volume, percent change, and the various lag variables for the weekly (top) and daily (bottom) data on the Dow Jones Industrial Average. The area of squares below the diagonal are proportional to the absolute value of the corresponding correlation coefficients."}

# Plots of correlations between variables: weekly data
par(mfrow = c(2, 1), mar = c(5, 4, 0.2, 2) + 0.1, mgp = c(2.3, 0.6, 0))
DJWeekly %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(method = "square", type = "lower", win.asp = 0.7, tl.cex = 0.7,
         tl.pos = "tp", tl.col = "black")
DJWeekly %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(add = TRUE, method = "number", type = "upper", 
         number.cex = 0.5, col = "black", tl.pos = "n", cl.pos = "n")

# Plots of correlations between variables: daily data
par(mar = c(5, 4, 0, 2) + 0.1)
DJDaily %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(method = "square", type = "lower", win.asp = 0.7, tl.cex = 0.7,
         tl.pos = "tp", tl.col = "black")
DJDaily %>% mutate(Direction = ifelse(Direction == "Up", 1, 0)) %>% cor() %>% 
corrplot(add = TRUE, method = "number", type = "upper", 
         number.cex = 0.5, col = "black", tl.pos = "n", cl.pos = "n")

```

``` {r Figure4, fig.height = 8, fig.cap = "\\label{fig:Figure4} Plots of S&P 500 weekly percent change against percent change of up to 5 weeks prior. Autocorrelation in weekly percent change corresponding to 1-5 week lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot percent change for given week against percent change up to five weeks ago
# Also plot autocorrelation in percent change
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(SPWeekly$PctChange, SPWeekly$Lag1, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (1-Week Lag)")
plot(SPWeekly$PctChange, SPWeekly$Lag2, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (2-Week Lag)")
plot(SPWeekly$PctChange, SPWeekly$Lag3, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (3-Week Lag)")
plot(SPWeekly$PctChange, SPWeekly$Lag4, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (4-Week Lag)")
plot(SPWeekly$PctChange, SPWeekly$Lag5, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (5-Week Lag)")
acf(SPWeekly$PctChange, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure5, fig.height = 8, fig.cap = "\\label{fig:Figure5} Plots of S&P 500 daily percent change against percent change of up to 5 days prior. Autocorrelation in daily percent change corresponding to 1-5 day lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot percent change for given day against percent change up to five days ago
# Also plot autocorrelation in percent change
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(SPDaily$PctChange, SPDaily$Lag1, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (1-Day Lag)")
plot(SPDaily$PctChange, SPDaily$Lag2, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (2-Day Lag)")
plot(SPDaily$PctChange, SPDaily$Lag3, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (3-Day Lag)")
plot(SPDaily$PctChange, SPDaily$Lag4, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (4-Day Lag)")
plot(SPDaily$PctChange, SPDaily$Lag5, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (5-Day Lag)")
acf(SPDaily$PctChange, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure6, fig.height = 8, fig.cap = "\\label{fig:Figure6} Plots of S&P 500 weekly volume against volume of up to 5 weeks prior. Autocorrelation in volume corresponding to 1-5 week lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot volume for given week against volume up to five weeks ago
# Also plot autocorrelation in volume
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(SPWeekly$BVolume, SPWeekly$VLag1, xlim = c(0, 40), xlab = "Volume", ylim = c(0, 40),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (1-Week Lag)")
plot(SPWeekly$BVolume, SPWeekly$VLag2, xlim = c(0, 40), xlab = "Volume", ylim = c(0, 40),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (2-Week Lag)")
plot(SPWeekly$BVolume, SPWeekly$VLag3, xlim = c(0, 40), xlab = "Volume", ylim = c(0, 40),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (3-Week Lag)")
plot(SPWeekly$BVolume, SPWeekly$VLag4, xlim = c(0, 40), xlab = "Volume", ylim = c(0, 40),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (4-Week Lag)")
plot(SPWeekly$BVolume, SPWeekly$VLag5, xlim = c(0, 40), xlab = "Volume", ylim = c(0, 40),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (5-Week Lag)")
acf(SPWeekly$BVolume, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure7, fig.height = 8, fig.cap = "\\label{fig:Figure7} Plots of S&P 500 daily volume against volume of up to 5 days prior. Autocorrelation in volume corresponding to 1-5 day lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot volume for given day against volume up to five days ago
# Also plot autocorrelation in volume
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(SPDaily$BVolume, SPDaily$VLag1, xlim = c(0, 12), xlab = "Volume", ylim = c(0, 12),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (1-Day Lag)")
plot(SPDaily$BVolume, SPDaily$VLag2, xlim = c(0, 12), xlab = "Volume", ylim = c(0, 12),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (2-Day Lag)")
plot(SPDaily$BVolume, SPDaily$VLag3, xlim = c(0, 12), xlab = "Volume", ylim = c(0, 12),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (3-Day Lag)")
plot(SPDaily$BVolume, SPDaily$VLag4, xlim = c(0, 12), xlab = "Volume", ylim = c(0, 12),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (4-Day Lag)")
plot(SPDaily$BVolume, SPDaily$VLag5, xlim = c(0, 12), xlab = "Volume", ylim = c(0, 12),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (5-Day Lag)")
acf(SPDaily$BVolume, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure8, fig.height = 8, fig.cap = "\\label{fig:Figure8} Plots of NASDAQ Composite weekly percent change against percent change of up to 5 weeks prior. Autocorrelation in weekly percent change corresponding to 1-5 week lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot percent change for given week against percent change up to five weeks ago
# Also plot autocorrelation in percent change
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(NDWeekly$PctChange, NDWeekly$Lag1, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (1-Week Lag)")
plot(NDWeekly$PctChange, NDWeekly$Lag2, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (2-Week Lag)")
plot(NDWeekly$PctChange, NDWeekly$Lag3, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (3-Week Lag)")
plot(NDWeekly$PctChange, NDWeekly$Lag4, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (4-Week Lag)")
plot(NDWeekly$PctChange, NDWeekly$Lag5, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (5-Week Lag)")
acf(NDWeekly$PctChange, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure9, fig.height = 8, fig.cap = "\\label{fig:Figure9} Plots of NASDAQ Composite daily percent change against percent change of up to 5 days prior. Autocorrelation in daily percent change corresponding to 1-5 day lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot percent change for given day against percent change up to five days ago
# Also plot autocorrelation in percent change
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(NDDaily$PctChange, NDDaily$Lag1, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (1-Day Lag)")
plot(NDDaily$PctChange, NDDaily$Lag2, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (2-Day Lag)")
plot(NDDaily$PctChange, NDDaily$Lag3, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (3-Day Lag)")
plot(NDDaily$PctChange, NDDaily$Lag4, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (4-Day Lag)")
plot(NDDaily$PctChange, NDDaily$Lag5, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (5-Day Lag)")
acf(NDDaily$PctChange, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure10, fig.height = 8, fig.cap = "\\label{fig:Figure10} Plots of NASDAQ Composite weekly volume against volume of up to 5 weeks prior. Autocorrelation in volume corresponding to 1-5 week lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot volume for given week against volume up to five weeks ago
# Also plot autocorrelation in volume
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(NDWeekly$BVolume, NDWeekly$VLag1, xlim = c(0, 28), xlab = "Volume", ylim = c(0, 28),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (1-Week Lag)")
plot(NDWeekly$BVolume, NDWeekly$VLag2, xlim = c(0, 28), xlab = "Volume", ylim = c(0, 28),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (2-Week Lag)")
plot(NDWeekly$BVolume, NDWeekly$VLag3, xlim = c(0, 28), xlab = "Volume", ylim = c(0, 28),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (3-Week Lag)")
plot(NDWeekly$BVolume, NDWeekly$VLag4, xlim = c(0, 28), xlab = "Volume", ylim = c(0, 28),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (4-Week Lag)")
plot(NDWeekly$BVolume, NDWeekly$VLag5, xlim = c(0, 28), xlab = "Volume", ylim = c(0, 28),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (5-Week Lag)")
acf(NDWeekly$BVolume, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure11, fig.height = 8, fig.cap = "\\label{fig:Figure11} Plots of NASDAQ Composite daily volume against volume of up to 5 days prior. Autocorrelation in volume corresponding to 1-5 day lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot volume for given day against volume up to five days ago
# Also plot autocorrelation in volume
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(NDDaily$BVolume, NDDaily$VLag1, xlim = c(0, 8), xlab = "Volume", ylim = c(0, 8),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (1-Day Lag)")
plot(NDDaily$BVolume, NDDaily$VLag2, xlim = c(0, 8), xlab = "Volume", ylim = c(0, 8),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (2-Day Lag)")
plot(NDDaily$BVolume, NDDaily$VLag3, xlim = c(0, 8), xlab = "Volume", ylim = c(0, 8),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (3-Day Lag)")
plot(NDDaily$BVolume, NDDaily$VLag4, xlim = c(0, 8), xlab = "Volume", ylim = c(0, 8),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (4-Day Lag)")
plot(NDDaily$BVolume, NDDaily$VLag5, xlim = c(0, 8), xlab = "Volume", ylim = c(0, 8),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (5-Day Lag)")
acf(NDDaily$BVolume, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure12, fig.height = 8, fig.cap = "\\label{fig:Figure12} Plots of Dow Jones Industrial Average weekly percent change against percent change of up to 5 weeks prior. Autocorrelation in weekly percent change corresponding to 1-5 week lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot percent change for given week against percent change up to five weeks ago
# Also plot autocorrelation in percent change
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(DJWeekly$PctChange, DJWeekly$Lag1, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (1-Week Lag)")
plot(DJWeekly$PctChange, DJWeekly$Lag2, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (2-Week Lag)")
plot(DJWeekly$PctChange, DJWeekly$Lag3, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (3-Week Lag)")
plot(DJWeekly$PctChange, DJWeekly$Lag4, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (4-Week Lag)")
plot(DJWeekly$PctChange, DJWeekly$Lag5, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (5-Week Lag)")
acf(DJWeekly$PctChange, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure13, fig.height = 8, fig.cap = "\\label{fig:Figure13} Plots of Dow Jones Industrial Average daily percent change against percent change of up to 5 days prior. Autocorrelation in daily percent change corresponding to 1-5 day lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot percent change for given day against percent change up to five days ago
# Also plot autocorrelation in percent change
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(DJDaily$PctChange, DJDaily$Lag1, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (1-Day Lag)")
plot(DJDaily$PctChange, DJDaily$Lag2, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (2-Day Lag)")
plot(DJDaily$PctChange, DJDaily$Lag3, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (3-Day Lag)")
plot(DJDaily$PctChange, DJDaily$Lag4, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (4-Day Lag)")
plot(DJDaily$PctChange, DJDaily$Lag5, xlim = c(-20, 20), xlab = "Percent Change", ylim = c(-20, 20),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Percent Change (5-Day Lag)")
acf(DJDaily$PctChange, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure14, fig.height = 8, fig.cap = "\\label{fig:Figure14} Plots of Dow Jones Industrial Average weekly volume against volume of up to 5 weeks prior. Autocorrelation in volume corresponding to 1-5 week lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot volume for given week against volume up to five weeks ago
# Also plot autocorrelation in volume
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(DJWeekly$BVolume, DJWeekly$VLag1, xlim = c(0, 0.04), xlab = "Volume", ylim = c(0, 0.04),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (1-Week Lag)")
plot(DJWeekly$BVolume, DJWeekly$VLag2, xlim = c(0, 0.04), xlab = "Volume", ylim = c(0, 0.04),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (2-Week Lag)")
plot(DJWeekly$BVolume, DJWeekly$VLag3, xlim = c(0, 0.04), xlab = "Volume", ylim = c(0, 0.04),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (3-Week Lag)")
plot(DJWeekly$BVolume, DJWeekly$VLag4, xlim = c(0, 0.04), xlab = "Volume", ylim = c(0, 0.04),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (4-Week Lag)")
plot(DJWeekly$BVolume, DJWeekly$VLag5, xlim = c(0, 0.04), xlab = "Volume", ylim = c(0, 0.04),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (5-Week Lag)")
acf(DJWeekly$BVolume, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure15, fig.height = 8, fig.cap = "\\label{fig:Figure15} Plots of Dow Jones Industrial Average daily volume against volume of up to 5 days prior. Autocorrelation in volume corresponding to 1-5 day lag times is shown in the bottom-right panel; dashed blue lines represent the 95% confidence interval for the autocorrelation."}

# Plot volume for given day against volume up to five days ago
# Also plot autocorrelation in volume
par(mfrow = c(3, 2), mar = c(4, 4, 2, 2) + 0.1)
plot(DJDaily$BVolume, DJDaily$VLag1, xlim = c(0, 0.008), xlab = "Volume", ylim = c(0, 0.008),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (1-Day Lag)")
plot(DJDaily$BVolume, DJDaily$VLag2, xlim = c(0, 0.008), xlab = "Volume", ylim = c(0, 0.008),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (2-Day Lag)")
plot(DJDaily$BVolume, DJDaily$VLag3, xlim = c(0, 0.008), xlab = "Volume", ylim = c(0, 0.008),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (3-Day Lag)")
plot(DJDaily$BVolume, DJDaily$VLag4, xlim = c(0, 0.008), xlab = "Volume", ylim = c(0, 0.008),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (4-Day Lag)")
plot(DJDaily$BVolume, DJDaily$VLag5, xlim = c(0, 0.008), xlab = "Volume", ylim = c(0, 0.008),
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylab = "Volume (5-Day Lag)")
acf(DJDaily$BVolume, lag.max = 5, lwd = 4, main = NA, xlab = "Lag (Weeks)",
    ylab = "Autocorrelation")
par(mfrow = c(1, 1))

```

``` {r Figure16, fig.height = 7.9, fig.cap = "\\label{fig:Figure16} Relative value of the S&P 500 over time compared to its 1986 value (top) and distribution of daily percent changes (bottom). Note that in the bottom panel, the leftmost bin represents all days with a percent change less than -5, and rightmost bin represents all days with a percent change greater than 5. Note that the top chart does not account for after-hours gains or losses."}

# Plot value of S&P 500 over time relative to a 1986 baseline
# Check this chart for accuracy, as it doesn't quite match up with the YF chart
# Discrepancies possibly due to after-hours trading?
par(mfrow = c(2, 1), mar = c(5, 4, 0.2, 2) + 0.1, mgp = c(2.3, 0.6, 0))
colourList <- replace(rep("green", nrow(SPWeekly)), which(SPWeekly$PctChange < 0), "red")
barplot(abs(SPWeekly$PctChange), col = colourList, border = colourList, ylim = c(0, 50), axes = FALSE)
par(new = TRUE)
totalReturn <- c()
for(i in 1:nrow(SPWeekly)){
  totalReturn <- c(totalReturn, prod((SPWeekly$PctChange[1:i]/100)+1))}
plot(1:nrow(SPWeekly), totalReturn, type = "l", ylim = c(0, 20), xaxt = "n", 
     xlab = NA, ylab = "Proportion of Initial Value")
axis(1, at = nrow(SPWeekly)*(0:7)/7, labels = seq(1986, 2021, length.out = 8))
title(xlab = "Year")

# Plot frequency of daily percent changes
labelvec <- (-11:11)/2
labelvec[labelvec %% 1 == 0.5] <- NA;
SPDaily %>% mutate(PctChange = ifelse(PctChange < -5, -5.1,
                                      ifelse(PctChange > 5, 5.1, PctChange))) %>%
  select(PctChange) %>%
  as.matrix() %>%
  as.vector() %>% 
  hist(x = ., breaks = (-11:11)/2, main = NA, xaxt = "n", xlab = "Daily Percent Change",
       col = c(colorRampPalette(c("red4", "red"), 0.5)(11), colorRampPalette(c("green", "green4"))(11)))
axis(1, at = (-11:11)/2, labels = labelvec)
box()

```

``` {r Figure17, fig.height = 7.9, fig.cap = "\\label{fig:Figure17} Relative value of the NASDAQ Composite over time compared to its 1986 value (top) and distribution of daily percent changes (bottom). Note that in the bottom panel, the leftmost bin represents all days with a percent change less than -5, and rightmost bin represents all days with a percent change greater than 5. Note that the top chart does not account for after-hours gains or losses."}

# Plot value of NASDAQ over time relative to a 1986 baseline
# Check this chart for accuracy, as it doesn't quite match up with the YF chart
# Discrepancies possibly due to after-hours trading?
par(mfrow = c(2, 1), mar = c(5, 4, 0.2, 2) + 0.1, mgp = c(2.3, 0.6, 0))
colourList <- replace(rep("green", nrow(NDWeekly)), which(NDWeekly$PctChange < 0), "red")
barplot(abs(NDWeekly$PctChange), col = colourList, border = colourList, ylim = c(0, 50), axes = FALSE)
par(new = TRUE)
totalReturn <- c()
for(i in 1:nrow(NDWeekly)){
  totalReturn <- c(totalReturn, prod((NDWeekly$PctChange[1:i]/100)+1))}
plot(1:nrow(NDWeekly), totalReturn, type = "l", ylim = c(0, 12), xaxt = "n", 
     xlab = NA, ylab = "Proportion of Initial Value")
axis(1, at = nrow(NDWeekly)*(0:7)/7, labels = seq(1986, 2021, length.out = 8))
title(xlab = "Year")

# Plot frequency of daily percent changes
labelvec <- (-11:11)/2
labelvec[labelvec %% 1 == 0.5] <- NA;
NDDaily %>% mutate(PctChange = ifelse(PctChange < -5, -5.1,
                                      ifelse(PctChange > 5, 5.1, PctChange))) %>%
  select(PctChange) %>%
  as.matrix() %>%
  as.vector() %>% 
  hist(x = ., breaks = (-11:11)/2, main = NA, xaxt = "n", xlab = "Daily Percent Change",
       col = c(colorRampPalette(c("red4", "red"), 0.5)(11), colorRampPalette(c("green", "green4"))(11)))
axis(1, at = (-11:11)/2, labels = labelvec)
box()

```

``` {r Figure18, fig.height = 7.9, fig.cap = "\\label{fig:Figure18} Relative value of the Dow Jones Industrial Average over time compared to its 1992 value (top) and distribution of daily percent changes (bottom). Note that in the bottom panel, the leftmost bin represents all days with a percent change less than -5, and rightmost bin represents all days with a percent change greater than 5. Note that the top chart does not account for after-hours gains or losses."}

# Plot value of Dow Jones over time relative to a 1986 baseline
# Check this chart for accuracy, as it doesn't quite match up with the YF chart
# Discrepancies possibly due to after-hours trading?
par(mfrow = c(2, 1), mar = c(5, 4, 0.2, 2) + 0.1, mgp = c(2.3, 0.6, 0))
colourList <- replace(rep("green", nrow(DJWeekly)), which(DJWeekly$PctChange < 0), "red")
barplot(abs(c(rep(0, 316), DJWeekly$PctChange)), col = colourList, border = colourList, ylim = c(0, 50), axes = FALSE)
par(new = TRUE)
totalReturn <- c()
for(i in 1:nrow(DJWeekly)){
  totalReturn <- c(totalReturn, prod((DJWeekly$PctChange[1:i]/100)+1))}
plot((nrow(SPWeekly) - nrow(DJWeekly) + 1):nrow(SPWeekly), totalReturn, type = "l", xlim = c(0, 1825), ylim = c(0, 12), xaxt = "n", 
     xlab = NA, ylab = "Proportion of Initial Value")
axis(1, at = nrow(SPWeekly)*(0:7)/7, labels = seq(1986, 2021, length.out = 8))
title(xlab = "Year")

# Plot frequency of daily percent changes
labelvec <- (-11:11)/2
labelvec[labelvec %% 1 == 0.5] <- NA;
DJDaily %>% mutate(PctChange = ifelse(PctChange < -5, -5.1,
                                      ifelse(PctChange > 5, 5.1, PctChange))) %>%
  select(PctChange) %>%
  as.matrix() %>%
  as.vector() %>% 
  hist(x = ., breaks = (-11:11)/2, main = NA, xaxt = "n", xlab = "Daily Percent Change",
       col = c(colorRampPalette(c("red4", "red"), 0.5)(11), colorRampPalette(c("green", "green4"))(11)))
axis(1, at = (-11:11)/2, labels = labelvec)
box()

```

# Predictive Analysis: S&P 500

## Logistic Regression

We begin our analyses on the S&P 500 data by examine how accurate a logistic regression can be for predicting whether the S&P 500 will increase or decrease in value. We first consider a model such that `Direction` is the response and `BVolume`, `Prev5GM`, `Prev5Pct`, and`Lag1` through `Lag5` are predictors.

``` {r, echo = TRUE, eval = FALSE}

glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + BVolume + Prev5GM + Prev5Pct,
    data = Weekly, family = binomial)

```

This model contains the maximum number of non-interactive predictors and is only one of many models that we can make; we could fit a model with only seven of these eight terms, or six of these eight terms, and so on. It is easy to show that the total number of non-interactive models that we can form is \begin{equation} \sum_{i=1}^{8}\frac{8!}{i!(8-i)!} \end{equation} which is equal to 255. Fitting these models individually like in the code above would be too time consuming, so we write a function `logreg.comb` to do it for us.

``` {r echo = TRUE, eval = FALSE}

logreg.comb <- function(dataTrain, dataTest, output, n){
  combn(names(dataTrain)[-c(2:3)], n) %>% 
    apply(FUN = paste0, MARGIN = 2, collapse = "+") %>% 
    paste0("Direction~", .) %>% 
    sapply(FUN = logreg, dataTrain = dataTrain, output = output) %>% 
    sort() %>%
    return()}

```

Here, we use the `combn` function to generate all 255 different combination of variables and then `paste0` to collapse them down into their individual formulas. We then use `sapply` to apply a custom function `logreg` to each of these formulas; this custom function fits a logistic regression using a given formula and training data set and returns a specified statistic such as accuracy, sensitivity, specificity, positive predictive value, or negative predictive value. Because this function is too long to properly include in a code snippet, we have not included it here and can instead be found in the markdown for this analysis. After using `logreg.comb` to find the model that maximises training set accuracy, we get the model \begin{equation} logit(\pi)=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\beta_{3}x_{3}+\beta_{4}x_{4}+\beta_{5}x_{5} \end{equation} with the predictor variables `Prev5GM`, `Prev5Pct`, `Lag1`, `Lag2`, and `Lag4`. This model has an accuracy of approximately 58.26% on the training data which, while low, still seems to be slightly better than guessing. However, we see that this accuracy is highly skewed towards predicting increases; our model has a training sensitivity of 98.64% and correctly classified 724 out of 734 increases, but has an abysmal specificity of 3.68% and correctly classified only 20 out of 543 decreases. Sensitivity and specificiy may not be useful in the context of this problem, though; recall that they would interpreted such that given an increase (or decrease), the sensitivity (or specificity) is the probability of correctly identifying that increase (or decrease). However, if we are trying to predict the behaviour of a stock or ETF next week, then we are not given an increase or decrease then since that is what we are trying to predict in the first place! As such, it might make more sense to examine something like positive predictive value (PPV); that is, when the model predicts an increase, the percentage of times that an increase actually occurs. If we use `logreg.comb` again, but this time seeking to maximise the training PPV, we actually get the same model, with a PPV of 58.06% on the training data. We can also try and maximise negative predictive value (NPV); that is, when the model predicts a decrease, the percentage of times that a decrease actually occurs. Upon doing so, we get the model \begin{equation} logit(\pi)=\beta_{0}+\beta_{1}x_{1} \end{equation} with `Prev5GM` as the only predictor. This model has an NPV of 100%, meaning that every time a decrease is predicted, the markets actually decrease. This number is suspiciously high, and after checking the confusion matrix, it is clear why: the model only predicted a decrease once and got it right, while the other 542 decreases were incorrectly predicted as increases. Even looking at the next few highest models in terms of NPV, we see the same trend of high NPVs due to a extremely low number of decreases that just so happened to be correctly predicted. As such, it might be wise to stick with accuracy and PPV when choosing our models.

We can also examine the training accuracy of models that include two-way interactions; the total number of two-way interaction terms plus single terms is 36, so thus the total number of interactive models that we can form is \begin{equation} \sum_{i=1}^{36}\frac{36!}{i!(36-i)!} \end{equation} which is equal to 68719476735. This would be far too computationally expensive for R, and we thus cannot calculate the accuracy of each model like we did when we only had 255 models. However, we can develop a heuristical solution that only searches through several of these possible models; we start with a backward step function that selects a model based on accuracy.

``` {r eval = FALSE, echo = TRUE}

# Function to remove a term and find new accuracy
logreg.lcv <- function(i, dataTrain, output){
  predVec[-i] %>% 
    paste0(., collapse = "+") %>% 
    paste0("Direction~", .) %>% 
    logreg(dataTrain = SPWeeklyTrain, output = output) %>% 
    return()}

# Function to return accuracy and new list of predictors after removing a predictor
logreg.bwd <- function(predVec, output){
  accs <- sapply(1:length(predVec), FUN = logreg.lcv, dataTrain = SPWeeklyTrain,
                 output = output)
  predVec <- predVec[-which.max(accs)]
  return(list(predVec = predVec, maxacc = max(accs)))}

```

Here, we start with the maximum number (36) of interactive and non-interactive terms, and use an `output` of `accuracy` to select our models based on overall prediction accuracy. We then calculate the new training accuracy after we remove a term, and do this for each term, then removing the term that results in the highest accuracy. We repeat this process until the model accuracy is maximised. Doing so results in a model with a training accuracy of 60.33% and 30 terms. We can also perform forward selection, starting with a null model and calculating the new training accuracy after adding a term, doing this for each term, and then adding the term that results in the highest accuracy.

``` {r eval = FALSE, echo = TRUE}

# Function to add a term and find new accuracy
logreg.lcv2 <- function(i, dataTrain, output){
  c(predVecFwd, predVecRemain[i]) %>% 
  paste0(., collapse = "+") %>% 
    paste0("Direction~", .) %>% 
    logreg(dataTrain = SPWeeklyTrain, output = output) %>% 
    return()}

# Function to return accuracy and new list of predictors after adding a predictor
logreg.fwd <- function(predVecRemain, predVecFwd, output){
  accs <- sapply(1:length(predVecRemain), FUN = logreg.lcv2, dataTrain = SPWeeklyTrain,
                 output = output)
  pvf <- c(predVecFwd, predVecRemain[which.max(accs)])
  pvr <- predVecRemain[-which.max(accs)]
  return(list(predVecFwd = pvf, predVecRemain = pvr, maxacc = max(accs)))}

```

Doing so now results in a model with a training accuracy of 59.15% and 9 terms; while the training accuracy is lower, there are fewer terms than our model resulting from backward selection. We can also use the same forward and backward algorithms but select models using PPV instead of accuracy. If we do so using the backward step algorithm, we get a model with a PPV of 60.30% and 26 terms; using the forward step algorithm, we get a model with a PPV of 59.38% and 12 terms.

``` {r include = FALSE, eval = FALSE}

# Note: Our analyses show that VLag1 - VLag5 doesn't make much of a difference
# Thus, we remove it to make the models simpler
select(SPWeeklyTrain, !(VLag1:VLag5)) -> SPWeeklyTrain
select(SPWeeklyTest, !(VLag1:VLag5)) -> SPWeeklyTest

# Function to perform logistic regression and output predictive accuracy
logreg <- function(formula, dataTrain, output){
  
  # Fit logistic regression using training data
  glm.fit <- glm(formula, data = dataTrain, family = "binomial")
  
  # Classify test data; get posterior probabilities
  glm.probs <- predict(glm.fit, dataTrain, type = "response")
  glm.pred <- rep("Down", length(glm.probs))
  glm.pred[glm.probs >= 0.5] <- "Up"
  
  # Confusion matrix of classified observations
  # Must manually add Down row if model never predicts Down
  confMatrix <- table(glm.pred, dataTrain$Direction)
  if(nrow(confMatrix) == 1){
    confMatrix <- rbind(c(0, 0), confMatrix)}
  rownames(confMatrix) <- c("[P]Down", "[P]Up")
  colnames(confMatrix) <- c("[O]Down", "[O]Up")
  
  # Overall predictive accuracy
  accuracy <- sum(diag(confMatrix)/sum(confMatrix))
  
  # Sensitivity and specificity
  specificity <- confMatrix[1, 1]/sum(confMatrix[, 1])
  sensitivity <- confMatrix[2, 2]/sum(confMatrix[, 2])
  
  # Positive and negative predictive value
  npv <- confMatrix[1, 1]/sum(confMatrix[1, ])
  ppv <- confMatrix[2, 2]/sum(confMatrix[2, ])
  
  # Return individual outputs or list of all outputs
  if(output == "all"){
    return(list(confMatrix = confMatrix, accuracy = accuracy, specificity = specificity,
              sensitivity = sensitivity, npv = npv, ppv = ppv))}
  if(output == "confMatrix"){
    return(confMatrix)}
  if(output == "accuracy"){
    return(accuracy)}
  if(output == "specificity"){
    return(specificity)}
  if(output == "sensitivity"){
    return(sensitivity)}
  if(output == "npv"){
    return(npv)}
  if(output == "ppv"){
    return(ppv)}}

```

``` {r include = FALSE, eval = FALSE}

# Function to fit logistic regression to all possible combinations of n variables
# Outputs value and combination of variables for desired prediction metric
logreg.comb <- function(dataTrain, output, n){
  combn(names(dataTrain)[-c(2:3)], n) %>% 
    apply(FUN = paste0, MARGIN = 2, collapse = "+") %>% 
    paste0("Direction~", .) %>% 
    sapply(FUN = logreg, dataTrain = dataTrain, output = output) %>% 
    sort() %>%
    return()}

# Find top 10 models for accuracy
SPWeeklyTop <- unlist(sapply(1:8, FUN = logreg.comb, dataTrain = SPWeeklyTrain, output = "accuracy"))
sort(SPWeeklyTop, decreasing = TRUE)[1:10]
logreg(Direction~Prev5GM+Prev5Pct+Lag1+Lag2+Lag4, dataTrain = SPWeeklyTrain, output = "confMatrix")

# Find top 10 models for ppv
SPWeeklyTop <- unlist(sapply(1:8, FUN = logreg.comb, dataTrain = SPWeeklyTrain, output = "ppv"))
sort(SPWeeklyTop, decreasing = TRUE)[1:10]

# Find top 10 models for npv
SPWeeklyTop <- unlist(sapply(1:8, FUN = logreg.comb, dataTrain = SPWeeklyTrain, output = "npv"))
sort(SPWeeklyTop, decreasing = TRUE)[1:10]
logreg(Direction ~ Prev5GM, dataTrain = SPWeeklyTrain, output = "confMatrix")

```

``` {r include = FALSE, eval = FALSE}

# Function to remove a term and find new accuracy
logreg.lcv <- function(i, dataTrain, output){
  predVec[-i] %>% 
    paste0(., collapse = "+") %>% 
    paste0("Direction~", .) %>% 
    logreg(dataTrain = SPWeeklyTrain, output = output) %>% 
    return()}

# Function to return accuracy and new list of predictors after removing a predictor
logreg.bwd <- function(predVec, output){
  accs <- sapply(1:length(predVec), FUN = logreg.lcv, dataTrain = SPWeeklyTrain, output = output)
  predVec <- predVec[-which.max(accs)]
  return(list(predVec = predVec, maxacc = max(accs)))}

# Function to add a term and find new accuracy
logreg.lcv2 <- function(i, dataTrain, output){
  c(predVecFwd, predVecRemain[i]) %>% 
  paste0(., collapse = "+") %>% 
    paste0("Direction~", .) %>% 
    logreg(dataTrain = SPWeeklyTrain, output = output) %>% 
    return()}

# Function to return accuracy and new list of predictors after adding a predictor
logreg.fwd <- function(predVecRemain, predVecFwd, output){
  accs <- sapply(1:length(predVecRemain), FUN = logreg.lcv2, dataTrain = SPWeeklyTrain, output = output)
  pvf <- c(predVecFwd, predVecRemain[which.max(accs)])
  pvr <- predVecRemain[-which.max(accs)]
  return(list(predVecFwd = pvf, predVecRemain = pvr, maxacc = max(accs)))}

# Apply backward step algorithm using accuracy
combn(names(SPWeeklyTrain)[-c(2:3)], 2) %>%
    apply(FUN = paste0, MARGIN = 2, collapse = ":") %>%
    c(names(SPWeeklyTrain)[-c(2:3)]) -> predVec
prevmaxacc <- c(0.5, 0.5)
maxacc <- logreg.bwd(predVec, "accuracy")$maxacc
predVec <- logreg.bwd(predVec, "accuracy")$predVec
prevmaxacc <- c(prevmaxacc, maxacc)
while(length(predVec) > 1){
  maxacc <- logreg.bwd(predVec, "accuracy")$maxacc
  if(maxacc < prevmaxacc[length(prevmaxacc) - 1]){
    break}
  predVec <- logreg.bwd(predVec, "accuracy")$predVec
  prevmaxacc <- c(prevmaxacc, maxacc)}
prevmaxacc[length(prevmaxacc)]

# Apply forward step algorithm using accuracy
combn(names(SPWeeklyTrain)[-c(2:3)], 2) %>%
    apply(FUN = paste0, MARGIN = 2, collapse = ":") %>%
    c(names(SPWeeklyTrain)[-c(2:3)]) -> predVecRemain
prevmaxacc <- c(0.5, 0.5)
predVecFwd <- c()
maxacc <- logreg.fwd(predVecRemain, predVecFwd, "accuracy")$maxacc
predVecFwd <- logreg.fwd(predVecRemain, predVecFwd, "accuracy")$predVecFwd
predVecRemain <- predVecRemain[predVecRemain != predVecFwd[length(predVecFwd)]]
prevmaxacc <- c(prevmaxacc, maxacc)
while(length(predVecFwd) < 36){
  maxacc <- logreg.fwd(predVecRemain, predVecFwd, "accuracy")$maxacc
  if(maxacc < prevmaxacc[length(prevmaxacc) - 1]){
    break}
  predVecFwd <- logreg.fwd(predVecRemain, predVecFwd, "accuracy")$predVecFwd
  predVecRemain <- predVecRemain[predVecRemain != predVecFwd[length(predVecFwd)]]
  prevmaxacc <- c(prevmaxacc, maxacc)}
prevmaxacc[length(prevmaxacc)]

# Apply backward step algorithm using PPV
combn(names(SPWeeklyTrain)[-c(2:3)], 2) %>%
    apply(FUN = paste0, MARGIN = 2, collapse = ":") %>%
    c(names(SPWeeklyTrain)[-c(2:3)]) -> predVec
prevmaxacc <- c(0.5, 0.5)
maxacc <- logreg.bwd(predVec, "ppv")$maxacc
predVec <- logreg.bwd(predVec, "ppv")$predVec
prevmaxacc <- c(prevmaxacc, maxacc)
while(length(predVec) > 1){
  maxacc <- logreg.bwd(predVec, "ppv")$maxacc
  if(maxacc < prevmaxacc[length(prevmaxacc) - 1]){
    break}
  predVec <- logreg.bwd(predVec, "ppv")$predVec
  prevmaxacc <- c(prevmaxacc, maxacc)}
prevmaxacc[length(prevmaxacc)]

# Apply forward step algorithm using PPV
combn(names(SPWeeklyTrain)[-c(2:3)], 2) %>%
    apply(FUN = paste0, MARGIN = 2, collapse = ":") %>%
    c(names(SPWeeklyTrain)[-c(2:3)]) -> predVecRemain
prevmaxacc <- c(0.5, 0.5)
predVecFwd <- c()
maxacc <- logreg.fwd(predVecRemain, predVecFwd, "ppv")$maxacc
predVecFwd <- logreg.fwd(predVecRemain, predVecFwd, "ppv")$predVecFwd
predVecRemain <- predVecRemain[predVecRemain != predVecFwd[length(predVecFwd)]]
prevmaxacc <- c(prevmaxacc, maxacc)
while(length(predVecFwd) < 36){
  maxacc <- logreg.fwd(predVecRemain, predVecFwd, "ppv")$maxacc
  if(maxacc < prevmaxacc[length(prevmaxacc) - 1]){
    break}
  predVecFwd <- logreg.fwd(predVecRemain, predVecFwd, "ppv")$predVecFwd
  predVecRemain <- predVecRemain[predVecRemain != predVecFwd[length(predVecFwd)]]
  prevmaxacc <- c(prevmaxacc, maxacc)}
prevmaxacc[length(prevmaxacc)]

```

``` {r eval = FALSE}

totalReturn <- c()
for(i in 1:nrow(SPWeekly)){
  totalReturn <- c(totalReturn, prod((SPWeekly$PctChange[1:i]/100)+1))}

```

``` {r eval = FALSE}

# Code above can be modified to get top 10 models for sensitivity, specificity, and NPV
# Not particularly useful, so they won't be used much in our analyses

# Get statistics from top 10 models in terms of accuracy
SPWeeklyTopA <- lapply(names(sort(SPWeeklyTop, decreasing = TRUE)[1:10]),
                       FUN = glm, data = SPWeeklyTrain, family = "binomial")

# Get coefficients from top 10 models
lapply(SPWeeklyTopA, FUN = coef) %>% 
  lapply(FUN = as.list) -> coeflist

# Create table of coefficients, AIC, and accuracy
SPWeeklyTopATab <- cbind(do.call(rbind.fill, lapply(coeflist, as.data.frame)),
                         sapply(SPWeeklyTopA, FUN = AIC),
                         as.numeric(sort(SPWeeklyTop, decreasing = TRUE)[1:10]))
names(SPWeeklyTopATab)[c(1, 9, 10, 11)] <- c("Intercept", "Lag1", "AIC", "Accuracy")

# Calculate maximum VIF for each model
sapply(sapply(SPWeeklyTopA, FUN = vif), FUN = max)

# Reorder table columns
SPWeeklyTopATab <- SPWeeklyTopATab[, c(1, 9, 2:4, 8, 5:7, 10:11)]

# Find top 10 models for PPV
SPWeeklyTop <- unlist(sapply(1:8, FUN = logreg.comb, dataTrain = SPWeeklyTrain,
                      dataTest = SPWeeklyTest, output = "ppv"))
sort(SPWeeklyTop, decreasing = TRUE)[1:10]

# Get statistics from top 10 models in terms of PPV
SPWeeklyTopP <- lapply(names(sort(SPWeeklyTop, decreasing = TRUE)[1:10]),
                       FUN = glm, data = SPWeeklyTrain, family = "binomial")

# Get coefficients from top 10 models
lapply(SPWeeklyTopP, FUN = coef) %>% 
  lapply(FUN = as.list) -> coeflist

# Create table of coefficients, AIC, max VIF, and accuracy
SPWeeklyTopPTab <- cbind(do.call(rbind.fill, lapply(coeflist, as.data.frame)),
                         sapply(SPWeeklyTopP, FUN = AIC),
                         as.numeric(sort(SPWeeklyTop, decreasing = TRUE)[1:10]))
names(SPWeeklyTopPTab)[c(1, 10, 11)] <- c("Intercept", "AIC", "PPV")

# Calculate maximum VIF for each model
sapply(sapply(SPWeeklyTopP, FUN = vif), FUN = max)

# Reorder table columns
SPWeeklyTopPTab <- SPWeeklyTopPTab[, c(1, 8, 2:3, 6:7, 9, 4, 5, 10:11)]

# Remove unneeded variables
remove(SPWeeklyTop, coeflist)

# Direction ~ Lag2 + Lag3 + Prev5Pct + Volume seems like the best candidate
# 2nd highest accuracy and highest PPV

```

``` {r eval = FALSE}

# Print tables of top 10 logistic models for accuracy and PPV
options(knitr.kable.NA = '')
kable(round(SPWeeklyTopATab, 3))
kable(round(SPWeeklyTopPTab, 3))

```

``` {r Figure19, eval = FALSE, fig.height = 8, fig.cap = "\\label{fig:Figure19} Standardised deviance residuals (top) and standardised Pearson residuals (bottom) for the best model in the logistic regression framework. Solid red lines represent a loess fit to the residuals and should be approximately horizontal at zero; dotted red lines represent $\\pm$ 2 standard deviations on the residuals."}

# Re-fit best model for easy plotting
glm.fit <- glm(Direction ~ Lag2 + Lag3 + Prev5Pct + Volume, data = SPWeeklyTrain, family = "binomial")

# Calculate standardised deviance and Pearson residuals for logistic regression
hvals <- influence(glm.fit)$hat
SRDeviance <- rstandard(glm.fit)
SRPearson <- residuals(glm.fit, "pearson")/sqrt(1 - hvals)

# Plot standardised deviance residuals
par(mfrow = c(2, 1), mar = c(5, 4, 0.2, 2) + 0.1, mgp = c(2.3, 0.6, 0))
plot(predict(glm.fit, type = "response"), SRDeviance, xlab = "Fitted Probability",
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylim = c(-3, 3),
     ylab = "Standardised Deviance Residuals")
lines(lowess(predict(glm.fit), residuals(glm.fit)), col = "red", lwd = 1.5)
abline(h = c(-2, 2), col = "red", lty = 3, lwd = 1.5)

# Plot standardised Pearson residuals
plot(predict(glm.fit, type = "response"), SRPearson, xlab = "Fitted Probability",
     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.3), pch = 16, ylim = c(-3, 3),
     ylab = "Pearson Deviance Residuals")
lines(lowess(predict(glm.fit), residuals(glm.fit, "pearson")), col = "red", lwd = 1.5)
abline(h = c(-2, 2), col = "red", lty = 3, lwd = 1.5)

```

## Discriminant Analysis

## K-Nearest Neighbours

## Support Vector Machines

# Predictive Analysis: NASDAQ Composite

## Logistic Regression

## Discriminant Analysis

## K-Nearest Neighbours

## Support Vector Machines

# Predictive Analysis: Dow Jones Industrial Average

## Logistic Regression

## Discriminant Analysis

## K-Nearest Neighbours

## Support Vector Machines

# Conclusions
